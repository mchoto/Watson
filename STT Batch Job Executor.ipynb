{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200804093425-0000\nKERNEL_ID = 211d9286-cb11-4639-8671-6209e08e70ec\n"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting watson-transformer\n  Downloading https://files.pythonhosted.org/packages/20/ff/daf740f4782b01f6926e1d92169ce7bae69e3e2fdf3c34599f51ef808fbf/watson_transformer-0.0.13-py3-none-any.whl\nCollecting ibm-cos-sdk-core>=2.4.3 (from watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/22/b44dfb4e45bf14fd04c6e3964b9c353de7a503c2de25848b2cc60a8bbc2d/ibm-cos-sdk-core-2.6.3.tar.gz (823kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 3.3MB/s eta 0:00:01\n\u001b[?25hCollecting botocore>=1.12.82 (from watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/a7/a00aa203bec250a202eda62de98185de3095abc3a9f7ffc052cb42acf5a3/botocore-1.17.34-py2.py3-none-any.whl (6.5MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.5MB 1.6MB/s eta 0:00:01\n\u001b[?25hCollecting ibm-watson>=4.4.0 (from watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/da/10f8774b319acdda29885931c01fae862622519bff492957c73b0ba84743/ibm-watson-4.5.0.tar.gz (370kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 378kB 4.5MB/s eta 0:00:01\n\u001b[?25hCollecting ibm-cos-sdk>=2.4.3 (from watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/3e/87c6df874801a8327d36821d742e96987e21276671d6b669574d31fed9be/ibm-cos-sdk-2.6.3.tar.gz (54kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 3.8MB/s eta 0:00:01\n\u001b[?25hCollecting ibm-cos-sdk-s3transfer>=2.4.3 (from watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/3e/f204ebdb2f9cb8f6fb431226e39cc9cb2875567d400cf7c5bc9c9b309a2c/ibm-cos-sdk-s3transfer-2.6.3.tar.gz (218kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 225kB 3.2MB/s eta 0:00:01\n\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from ibm-cos-sdk-core>=2.4.3->watson-transformer)\n  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\nCollecting docutils<0.16,>=0.10 (from ibm-cos-sdk-core>=2.4.3->watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 552kB 4.2MB/s eta 0:00:01\n\u001b[?25hCollecting requests<3.0,>=2.18 (from ibm-cos-sdk-core>=2.4.3->watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 3.1MB/s eta 0:00:01\n\u001b[?25hCollecting python-dateutil<3.0.0,>=2.1 (from ibm-cos-sdk-core>=2.4.3->watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235kB 4.8MB/s eta 0:00:01\n\u001b[?25hCollecting urllib3<1.26,>=1.20; python_version != \"3.4\" (from botocore>=1.12.82->watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 5.3MB/s eta 0:00:01\n\u001b[?25hCollecting websocket-client==0.48.0 (from ibm-watson>=4.4.0->watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl (198kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 5.1MB/s eta 0:00:01\n\u001b[?25hCollecting ibm_cloud_sdk_core==1.5.1 (from ibm-watson>=4.4.0->watson-transformer)\n  Downloading https://files.pythonhosted.org/packages/b7/f6/10d5271c807d73d236e6ae07b68035fed78b28b5ab836704d34097af3986/ibm-cloud-sdk-core-1.5.1.tar.gz\nCollecting idna<3,>=2.5 (from requests<3.0,>=2.18->ibm-cos-sdk-core>=2.4.3->watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 2.6MB/s eta 0:00:01\n\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3.0,>=2.18->ibm-cos-sdk-core>=2.4.3->watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 3.7MB/s eta 0:00:01\n\u001b[?25hCollecting chardet<4,>=3.0.2 (from requests<3.0,>=2.18->ibm-cos-sdk-core>=2.4.3->watson-transformer)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 5.1MB/s eta 0:00:01\n\u001b[?25hCollecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core>=2.4.3->watson-transformer)\n  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\nCollecting PyJWT>=1.7.1 (from ibm_cloud_sdk_core==1.5.1->ibm-watson>=4.4.0->watson-transformer)\n  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\nBuilding wheels for collected packages: ibm-cos-sdk-core, ibm-watson, ibm-cos-sdk, ibm-cos-sdk-s3transfer, ibm-cloud-sdk-core\n  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/f1/29/05/ab91fed96d22f69b37c4331f33b77f0f05df0c382bf5f09167\n  Building wheel for ibm-watson (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/71/9a/0a/9b3ca8eca69bc5362eb04709a750b30055a9d27818fd0c9494\n  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/23/c0/0b/6e7df83370c0fd314b4fe9aef6667e40178b31e7180dc69f26\n  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/89/7d/0a/2ff0215b826ef7d5a5292a62d0f57144976d7dc61fe57f2445\n  Building wheel for ibm-cloud-sdk-core (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/spark/shared/.cache/pip/wheels/6a/42/50/f96888116b329578304f9dda4693cef6f3e76e18272d22cb6c\nSuccessfully built ibm-cos-sdk-core ibm-watson ibm-cos-sdk ibm-cos-sdk-s3transfer ibm-cloud-sdk-core\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n\u001b[31mwatson-machine-learning-client-v4 1.0.95 has requirement ibm-cos-sdk==2.6.0, but you'll have ibm-cos-sdk 2.6.3 which is incompatible.\u001b[0m\n\u001b[31mpytest-openfiles 0.5.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mpytest-doctestplus 0.7.0 has requirement pytest>=4.0, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mpytest-astropy 0.8.0 has requirement pytest>=4.6, but you'll have pytest 3.10.1 which is incompatible.\u001b[0m\n\u001b[31mboto3 1.9.82 has requirement botocore<1.13.0,>=1.12.82, but you'll have botocore 1.17.34 which is incompatible.\u001b[0m\nInstalling collected packages: jmespath, docutils, idna, certifi, urllib3, chardet, requests, six, python-dateutil, ibm-cos-sdk-core, botocore, websocket-client, PyJWT, ibm-cloud-sdk-core, ibm-watson, ibm-cos-sdk-s3transfer, ibm-cos-sdk, watson-transformer\nSuccessfully installed PyJWT-1.7.1 botocore-1.17.34 certifi-2020.6.20 chardet-3.0.4 docutils-0.15.2 ibm-cloud-sdk-core-1.5.1 ibm-cos-sdk-2.6.3 ibm-cos-sdk-core-2.6.3 ibm-cos-sdk-s3transfer-2.6.3 ibm-watson-4.5.0 idna-2.10 jmespath-0.10.0 python-dateutil-2.8.1 requests-2.24.0 six-1.15.0 urllib3-1.25.10 watson-transformer-0.0.13 websocket-client-0.48.0\n"
                }
            ],
            "source": "!pip install watson-transformer"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "from watson_transformer import WatsonServiceTransformer, JSONTransformer, FlatColumnTransformer\nfrom watson_transformer.service import STT, NLU\nfrom watson_transformer.contrib.stt import DefaultSTTParser\nfrom watson_transformer.contrib.nlu import DefaultNLUParser\nimport watson_transformer.contrib.readers as readers\nfrom ibm_watson.natural_language_understanding_v1 import Features, KeywordsOptions, ConceptsOptions, SentimentOptions, EmotionOptions\n\nimport json\nfrom ibm_watson import SpeechToTextV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport time\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.ml import Pipeline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Watson Transformer package - Job execution\n###  Batch job execution for STT trancriptions\n###  Red items need to be substitted for execution"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## [Part 1: Define services and transformers](#pt1)\n\n## [Part 2: Pipeline & Execution](#pt2)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt1'></a> \n## Part 1: Define services and transformers\n\n1. [Enter Service Credentials](#pt1_s1)\n2. [Load file names to read](#pt1_s2)\n    - [from Object Storage](#pt1_s2a)\n    - [from project file](#pt1_s2b) \n3. [Create reader for IBM Cloud Object Storage](#pt1_s3)\n4. [Define Watson Service & Initialize transformers](#pt1_s4)\n   "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt1_s1'></a> \n###  <p style=\"color:Red\">  1. Enter Service Credentials </p>"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": "# Credentials to cloud object storage\n\napi_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nauth_endpoint = 'https://iam.ng.bluemix.net/oidc/token'\nprivate_endpoint_url='https://s3.us-south.objectstorage.service.networklayer.com' \npublic_endpoint_url='https://s3.us-south.objectstorage.softlayer.net' \ninstance_id = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nBucket ='stt-samples'"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": "#STT Credentials \n\nstt_token = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" \nstt_endpoint = 'https://stream.watsonplatform.net/speech-to-text/api'"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": "import boto3\nimport types\nfrom botocore.client import Config\nimport ibm_boto3\n\ncos_clstorage  = ibm_boto3.resource(service_name='s3',\n                               ibm_api_key_id=api_key,\n                               ibm_auth_endpoint=auth_endpoint,\n                               config=Config(signature_version='oauth'),\n                               endpoint_url=public_endpoint_url,\n                               ibm_service_instance_id= instance_id)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt1_s2'></a> \n### 2. Load Data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt1_s2a'></a> \n### <p style=\"color:Red\"> Option A) Load data file names from object storage (change file extension if necessary) </p>"
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": "#Function iterates over IBM COS and appends every object into a list, and then adds them to a dataframe to use later \n#enter extension to keep for files - audio_ext=None if you want to retrieve everything - 'mp3' for ending in .mp3, etc \n\ndef get_bucket_contents(bucket_name,audio_ext=None):\n    files_list=[]\n    try: \n        files = cos_clstorage.Bucket(bucket_name).objects.all()\n        for file in files:\n            #print(\"Item: {0} ({1} bytes).\".format(file.key, file.size))\n            files_list.append(file.key)      \n    \n    except Exception as e:\n        print(\"Unable to retrieve bucket contents: {0}\".format(e))\n    \n    #dataframe with all the available files in the object storage (even call logs)\n    bucket_files = pd.DataFrame({'audio_file': files_list})\n    if audio_ext == None:\n        audio_files = bucket_files\n    else: \n        audio_files = bucket_files[bucket_files['audio_file'].str.contains(audio_ext)]\n    return audio_files"
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": "audio_files = get_bucket_contents(Bucket,audio_ext='mp3') "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt1_s2a'></a> \n### <p style=\"color:Red\"> Option B) Load data file names from object storage (change file name if importing from project)</p>"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "audio_file    20\ndtype: int64\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio_file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>incomingcall_sample1.mp3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>incomingcall_sample2.mp3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>incomingcall_sample3 copy 2.mp3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>incomingcall_sample3 copy 3.mp3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>incomingcall_sample3 copy.mp3</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                        audio_file\n0         incomingcall_sample1.mp3\n1         incomingcall_sample2.mp3\n2  incomingcall_sample3 copy 2.mp3\n3  incomingcall_sample3 copy 3.mp3\n4    incomingcall_sample3 copy.mp3"
                    },
                    "execution_count": 30,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "mp3_project_files = pd.read_csv(project.get_file('MP3_AudioFiles.csv'), sep=',')\nprint(mp3_project_files.count())\nmp3_project_files.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt1_s3'></a> \n### 3. Create reader for IBM COS"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "import types\nfrom botocore.client import Config\nimport ibm_boto3\ndef __iter__(self): return 0\n\n\"\"\"\n\"\n\" IBM COS reader\n\"\n\"\"\"\n        \ndef ibm_cos_reader(audio_file, maximum_size=200*1024*1024):\n    # boto3 is not thread safe\n    # issue: https://github.com/boto/botocore/issues/1246\n    # ref: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html\n    # create new session for each thread, boto3 is not thread safe\n    session = ibm_boto3.session.Session() \n    cos_client = session.client(service_name='s3',\n                                ibm_api_key_id=api_key,\n                                ibm_auth_endpoint=auth_endpoint,\n                                config=Config(signature_version='oauth'),\n                                endpoint_url=public_endpoint_url)\n    # read file object\n    file_obj = cos_client.get_object(Bucket=Bucket, Key=audio_file)\n    # make sure file size is less than maximum size\n    if int(file_obj['ContentLength']) >= maximum_size:\n        return None\n    else:\n        audio_stream = file_obj['Body']\n        if not hasattr(audio_stream, \"__iter__\"): audio_stream.__iter__ = types.MethodType( __iter__, audio_stream)\n        return audio_stream"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt1_s4'></a> \n### <p style=\"color:Red\"> 4. Define Watson Service & Initialize Transformers: this is where you are defining number of worker nodes </p>"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "#defining STT service\nstt = STT(token = stt_token,\n                   endpoint = stt_endpoint,\n                   reader = ibm_cos_reader,\n                   model='en-US_ShortForm_NarrowbandModel',\n                   inactivity_timeout=-1,\n                   profanity_filter=False,\n                   max_alternatives=1,\n                   split_transcript_at_phrase_end=False,\n                   content_type='audio/mp3')"
        },
        {
            "cell_type": "code",
            "execution_count": 124,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "wt = WatsonServiceTransformer(inputCol='audio_file', \n                               outputCol='stt_response',\n                               vectorization=True,\n                               max_workers =20,\n                               service=stt)"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "jt_stt = JSONTransformer(inputCol='stt_response',\n                     outputCol='transcript',\n                     removeInputCol=False,\n                     parser = DefaultSTTParser())\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt2'></a> \n\n## Part 2: Pipeline & Execution\n1. [Create PySpark DataFrame and pipeline](#pt2_s1)\n2. [Define needed variables](#pt2_s2)\n3. [Logger](#pt2_s3)\n4. [Batch Runner](#pt2_s4)\n5. [Launch Pad](#pt2_5)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt2_s1'></a> \n### <p style=\"color:Red\"> 1.  Create PySpark DataFrame and pipeline:</p> \n    substitute \"mp4_project_files\" for dataframe name containing the audio files\n"
        },
        {
            "cell_type": "code",
            "execution_count": 119,
            "metadata": {},
            "outputs": [],
            "source": "foo = spark.createDataFrame(mp3_project_files).repartition(5)\npipeline_stt = Pipeline(stages=[wt, jt_stt])"
        },
        {
            "cell_type": "code",
            "execution_count": 122,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "== Physical Plan ==\n*(2) Project [audio_file#119, stt_response#122, pythonUDF0#131 AS transcript#126]\n+- BatchEvalPython [DefaultSTTParser(stt_response#122)], [audio_file#119, stt_response#122, pythonUDF0#131]\n   +- *(1) Project [audio_file#119, pythonUDF0#130 AS stt_response#122]\n      +- ArrowEvalPython [vectorized_udf(audio_file#119)], [audio_file#119, pythonUDF0#130]\n         +- Exchange RoundRobinPartitioning(5)\n            +- Scan ExistingRDD[audio_file#119]\n"
                }
            ],
            "source": "fp = pipeline_stt.fit(foo)\nfp.transform(foo).explain()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt2_s2'></a> \n### <p style=\"color:Red\">  2. Define Variables (choose your own variables) </p> "
        },
        {
            "cell_type": "code",
            "execution_count": 114,
            "metadata": {},
            "outputs": [],
            "source": "df = mp3_project_files #pandas dataframe containing list of file names \nbatch_size=5000 #number of files to process in each batch if there are too many files\nnum_partitions=250 #number of partitions per batch (partititions = batch_size/max_workers)\nlog_csv_name='company_pipeline_log1.csv' #it will only create one log per job - remember to include .csv \nfile_prefix='company_transcription_part_2'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt2_s3'></a> \n## 3. Logger"
        },
        {
            "cell_type": "code",
            "execution_count": 115,
            "metadata": {},
            "outputs": [],
            "source": "import sys\nimport logging\nfrom datetime import datetime\nlogger = logging.getLogger(__name__)"
        },
        {
            "cell_type": "code",
            "execution_count": 116,
            "metadata": {},
            "outputs": [],
            "source": "\"\"\"\n\"\n\" write log to file\n\"\n\"\"\"\ndef project_logger(filename, info, new_log_file=False):\n    timestamp = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n    \n    if new_log_file:\n        try:\n            # create empty log file\n            foo = pd.DataFrame({\"timestamp\":[timestamp], \"log\":[info]})\n            project.save_data(filename, foo.to_csv(index=False, sep='|'), overwrite=True)\n        except:\n            pass\n    else:  \n        try:\n            df = pd.read_csv(project.get_file(filename), sep='|')\n            df.loc[df.shape[0]] = [timestamp, info]\n            project.save_data(filename, df.to_csv(index=False, sep='|'), overwrite=True)\n        except:\n            pass\n        "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt2_s4'></a> \n## 4. Batch Runner"
        },
        {
            "cell_type": "code",
            "execution_count": 117,
            "metadata": {},
            "outputs": [],
            "source": "\"\"\"\n\"\n\" functiom provide another layer of fault resistance on top of Spark engine.\n\"\n\"\"\"\ndef batch_runner(df, pipeline, project, \n                               batch_start=1, \n                               batch_size=batch_size, \n                               num_partitions=num_partitions,\n                               file_prefix=None,\n                               log_csv_name=log_csv_name,\n                               overwrite=False,\n                               verbose=True):\n    \"\"\"\n    @param::df: the input pandas dataframe\n    @param::pipeline: the fitted Spark ML pipeline\n    @param::project: the Watson project instance\n    @param::batch_start: the row pointer to where the batch process begin\n    @param::batch_size: the size of the data batch\n    @param::num_partitions: the number of partitions\n    @param::file_prefix: the prefix of the output data file\n    @param::overwrite: whether or not overwrite the output data file when it already exist\n    @param::verbose: toggle log info\n    return: None\n    \"\"\"\n    if file_prefix == None:\n        file_prefix = str(uuid.uuid4())\n    \n    row_count = df.shape[0]\n    i = batch_start - 1\n    iteration = 1\n    not_finished = True\n    \n    while not_finished:\n        batch_df = df.iloc[i:i+batch_size]\n        if i+batch_size >= row_count:\n            not_finished = False  \n        try:\n            sdf = spark.createDataFrame(batch_df).repartition(num_partitions)\n            start = time.perf_counter()\n            tmp = pipeline.transform(sdf).toPandas()\n            delta = time.perf_counter() - start\n            filename = '%s_result_batch_%s.csv'%(file_prefix, iteration)\n            project.save_data(filename, tmp.to_csv(index=False, sep='|'), overwrite=overwrite)\n            if verbose:\n                print('> batch %s of %s rows finished process in %d seconds.'%(iteration, batch_df.shape[0], delta))\n                project_logger(log_csv_name, 'Batch %s of %s rows finished process in %d seconds.'%(iteration, batch_df.shape[0], delta))\n                \n        except Exception as e:\n            error_filename = '%s_failed_batch_%s.csv'%(file_prefix, iteration)\n            project.save_data(error_filename, batch_df.to_csv(index=False, sep='|'), overwrite=overwrite)\n            print('> batch %s failed: %s'%(iteration, e))\n            project_logger(log_csv_name, 'Batch %s failed: %s'%(iteration, e)) \n        finally:\n            i += batch_size\n            iteration += 1"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a id='pt2_5'></a> \n## 5. Launch pad"
        },
        {
            "cell_type": "code",
            "execution_count": 118,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "> the input size of dataframe:  (20, 1)\n> the batch runner start executing...\n> batch 1 of 20 rows finished process in 7 seconds.\n"
                }
            ],
            "source": "\"\"\"\n\"\n\" launch pad\n\"\n\"\"\"\n\n\nprint('> the input size of dataframe: ', df.shape)\nproject_logger(log_csv_name, \"*** Pipeline is starting ....\", new_log_file=True)  \nproject_logger(log_csv_name, \"The input size of dataframe: (%s, %s)\"%(df.shape))  \ndf.head(3)\n\n\n# execute the batch runner\nprint('> the batch runner start executing...')\nproject_logger(log_csv_name, \"The batch runner start executing...\") \n\nbatch_runner(df, fp, project, batch_size=batch_size, num_partitions=num_partitions, file_prefix=file_prefix, overwrite=True)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}